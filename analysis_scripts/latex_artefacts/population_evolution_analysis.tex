% Population Evolution and Strategic Behavior Analysis
% Generated from IPD Tournament Results
%
% This file contains concise analysis of population evolution patterns
% and strategic behavior insights from the 8-experiment tournament series

\section*{Population Evolution Analysis}

Population evolution becomes increasingly pronounced as shadow length increases from 5\% to 75\%, driven by fundamental game-theoretic dynamics where shorter games create stronger evolutionary pressure. The 75\% shadow condition produces the shortest expected games (1.33 rounds) where immediate payoff maximization dominates, leading to severe evolutionary pressure that eliminates most agents by Phase 4 as only the most exploitative strategies survive the rapid elimination tournaments. In contrast, the 5\% shadow tournaments create the longest expected games (20 rounds) where reputation, reciprocity, and long-term strategic thinking become paramount, resulting in stable populations across all five phases as cooperative and adaptive strategies can demonstrate their superiority over purely exploitative approaches. The 10\% and 25\% shadow conditions show intermediate patterns—25\% creates medium-length games (4 rounds) that begin enabling reputation-based strategies but still favor exploitation, while 10\% produces long games (10 rounds) that maintain relatively stable populations as sophisticated strategies have sufficient time to establish cooperative equilibria. This demonstrates that evolutionary pressure in IPD tournaments is inversely related to game length: shorter interactions eliminate strategies that require time to build reputation or learn opponent patterns.

\section*{Strategic Behavior Insights for Long to Very Long Game Lengths}

Analysis of the 5\%, 10\%, and 25\% shadow length tournaments reveals distinct strategic patterns as game horizons extend from medium interactions (4 rounds) to very long games (20 rounds). Large language model performance varies significantly across providers and temperature settings, with longer games generally enabling more sophisticated strategic reasoning. Claude4-Sonnet demonstrates remarkable consistency and improvement with longer games, maintaining scores between 2.6-2.8 in all conditions regardless of temperature (T=0.2, 0.5, 0.8), suggesting robust strategic reasoning that adapts well to extended interactions. Mistral-Medium shows dramatic improvement as games lengthen: poor performance in shorter interactions is overcome in longer games where T=0.7 and T=1.2 consistently outperform T=0.2, indicating that this model requires extended interaction time for effective strategic adaptation. Gemini20Flash exhibits steady performance across all game lengths with scores clustering around 2.5-2.8, demonstrating reliable strategic reasoning that benefits from but does not require extended game horizons. OpenAI models display the most interesting patterns—GPT5mini maintains consistently strong performance (2.6-2.9 range) across all game lengths, while GPT5nano shows marked improvement from medium to long games, suggesting different strategic adaptation capabilities within the GPT family.

Adaptive strategies demonstrate pronounced performance improvements that directly correlate with game length, validating the theoretical expectation that learning algorithms require time to gather information and optimize strategies. Q-Learning shows dramatic improvement from medium games (2.977 in 25\% shadow) to very long games, while Thompson Sampling similarly benefits from extended interactions, achieving its best performance (2.968) in 25\% conditions where sufficient rounds enable effective exploration-exploitation balance. Gradient Meta-Learner exhibits the strongest correlation with game length, improving steadily from 2.626 in very long games to 2.921 in medium games, demonstrating that meta-learning approaches require substantial interaction history to identify and adapt to opponent patterns effectively.

Temperature effects across LLM providers reveal nuanced strategic sensitivities that become more pronounced in longer games where strategic consistency matters more than short-term exploitation. Claude4-Sonnet shows minimal temperature sensitivity across all game lengths, with T=0.8 providing slight advantages in longer interactions, suggesting that moderate randomness aids strategic flexibility without compromising long-term reasoning. Mistral-Medium exhibits the most pronounced temperature effects, with T=0.2 consistently underperforming in all game lengths, while T=0.7 and T=1.2 show optimal performance in different conditions, indicating that this model's strategic reasoning is highly sensitive to temperature-controlled exploration. Gemini20Flash maintains relatively stable performance across temperature ranges in all game lengths, though T=0.7 consistently provides optimal results, suggesting an ideal balance between exploration and exploitation for this model's strategic reasoning.

Memory mode comparison between anonymous and opponent tracking reveals substantial strategic advantages that become more pronounced in longer games where accumulated information enables sophisticated counter-strategies. In opponent tracking mode, all agent types maintain more stable performance across phases, with classical strategies like TFT showing less variation and improved consistency (2.717-2.848 range in 10\% tracking vs 2.754-2.850 in anonymous mode). LLM strategies benefit dramatically from opponent information across all game lengths—Claude4-Sonnet scores improve and stabilize when opponent histories are available, while Mistral and Gemini models show enhanced strategic adaptation. The tracking advantage becomes most pronounced in the longest games (5\% shadow), where 20-round interactions allow sophisticated pattern recognition and counter-strategy development that would be impossible in anonymous mode, demonstrating the critical importance of information availability for advanced strategic reasoning.

\section*{Large Language Model Performance in Extended Interactions}

Cross-model analysis of LLM performance in the 5\%, 10\%, and 25\% shadow tournaments (20, 10, and 4 round games respectively) reveals distinct strategic capabilities and adaptation patterns. GPT5mini emerges as the strongest overall performer with an average score of 2.731 across all long-game conditions, demonstrating consistent strategic reasoning regardless of game length or memory mode. This model maintains remarkably stable performance ranging from 2.627 to 2.787 across different shadow conditions, suggesting robust strategic heuristics that adapt effectively to varying interaction horizons. GPT4.1mini achieves the second-highest overall average (2.573) and shows particular strength in the longest games (5\% shadow: 2.810 anonymous, 2.672 tracking), indicating sophisticated long-term strategic planning capabilities that benefit from extended interaction sequences.

Claude4-Sonnet demonstrates exceptional consistency across all experimental conditions with an overall average of 2.569, exhibiting minimal performance variation between anonymous (2.670) and tracking (2.690) modes in 10\% shadow games. This stability suggests that Claude's strategic reasoning is largely self-contained and does not rely heavily on opponent modeling, in contrast to other models that show greater sensitivity to information availability. Gemini20Flash maintains steady performance (2.552 overall) with slight improvements in tracking modes, particularly in longer games where opponent information becomes more valuable for strategic adaptation.

Mistral-Medium exhibits the most pronounced dependency on opponent information, with dramatic performance improvements in tracking mode across all game lengths. In 25\% shadow games, Mistral improves from 2.440 in anonymous mode to 2.606 in tracking mode—a 0.166 point gain that represents the largest memory mode effect observed among all LLM models. This pattern suggests that Mistral's strategic reasoning architecture is particularly well-suited to opponent modeling and counter-strategy development, but struggles when forced to operate without historical opponent data. The model's overall performance (2.355) places it last among LLM competitors, yet its tracking mode capabilities demonstrate sophisticated strategic adaptation when information is available.

Adaptive strategies show interesting performance patterns that complement LLM behavior in extended interactions. Thompson Sampling achieves exceptional performance in medium-length games (25\% shadow: 2.836 anonymous, 2.816 tracking), outperforming all LLM models in these conditions where exploration-exploitation balance becomes critical. Q-Learning similarly benefits from extended interactions, improving from 2.126 in very long games to 2.793 in medium games, suggesting that moderate game lengths provide optimal conditions for reinforcement learning approaches. These results indicate that while LLMs excel in consistent strategic reasoning, adaptive algorithms can surpass them in specific game length conditions where their learning mechanisms align with interaction dynamics.

Temperature effects across the 5\%, 10\%, and 25\% shadow experiments do not reveal extremely pronounced performance differences within individual LLM models. Claude4-Sonnet maintains consistent performance across temperature settings (T=0.2, 0.5, 0.8) with variations typically under 0.1 points, while Gemini20Flash similarly shows stable behavior across its temperature range (T=0.2, 0.7, 1.2). Mistral-Medium exhibits the most temperature sensitivity, with T=0.2 consistently underperforming relative to T=0.7 and T=1.2, though these differences remain modest compared to the larger performance gaps between different models and memory modes.
